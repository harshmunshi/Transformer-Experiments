# Transformer-Experiments

This repo is for a personal understanding of each and every small mechanism that goes into the nuts and bolts of tansformer based architectures, starting with the OG of course.
What do we intend to do?

* Try to understand what a transformer does?
* Coding the paper "Attention is all you need"
* Langchain and RAG applications
* Moving away from texts and understanding the effect on CV
* Understanding ViT
* Modifying ViT with shallower representation (enter the OG CNNs)
* Make a generic face encoding pipeline with transformers (like MARLIN)
* Cross attention
* Making a synthetic media pipeline with audio and video cross attention
